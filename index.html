<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-MYG8Y8V6GQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "G-MYG8Y8V6GQ");
  </script>
  <title>Yuanxing Zhang</title>
  <!-- Meta -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="" />
  <meta name="author" content="Xiaoying Riley at 3rd Wave Media" />

  <link href="https://fonts.googleapis.com/css?family=Lato:300,400,300italic,400italic" rel="stylesheet"
    type="text/css" />
  <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />

  <!-- FontAwesome JS -->
  <script defer src="assets/fontawesome/js/all.js"></script>

  <!-- Global CSS -->
  <link rel="stylesheet" href="assets/plugins/bootstrap/css/bootstrap.min.css" />

  <!-- github calendar css -->
  <!-- <link rel="stylesheet" href="assets/plugins/github-calendar/dist/github-calendar-responsive.css"> -->
  <!-- github activity css -->
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/2.0.2/octicons.min.css"> -->
  <!-- <link rel="stylesheet" href="assets/plugins/github-activity/src/github-activity.css"> -->

  <!-- Theme CSS -->
  <link id="theme-style" rel="stylesheet" href="assets/css/styles.css" />
</head>

<body>
  <!-- ******HEADER****** -->
  <header class="header">
    <div class="container">
      <div class="row align-items-center">
        <div class="col">
          <!-- <img class="profile-image img-fluid float-start rounded-circle" src="images/me2.jpg" alt="profile image" /> -->
          <img class="profile-image img-fluid float-start rounded-circle" src="images/me2.jpg" alt="profile image"/>
          <div class="profile-content">
            <h1 class="name">Yuanxing Zhang (张远行)</h1>
            <!-- <h2 class="desc">Web App Developer</h2> -->
            <div class="content">
              <!-- https://github.com/KwaiVGI -->
              I am currently a Senior Staff Researcher at Kling Team,
              <a href="https://www.kuaishou.com/en" target="_blank">Kuaishou Technology</a>, leading the
              <b>multimodal understanding</b> group, covering
              MLLM foundation model, omni-modal captioning, prompt enhancement, multimodal representation, reward model, and understanding for unified model.<br/>
              <font color="orange"><b>We are actively looking for research interns and full-time
                  researchers to work on cutting-edge research topics</b></font>. If you're interested in exploring these opportunities, please reach out to me at
              <font color="orange"><a href="mailto:longo11070001@gmail.com">longo11070001@gmail.com</a></font>.
              <br/>
            </div>
            <div class="content">
              <br/>
              <p align="left">
                I earned my Ph.D. from Peking University in 2020, where I was advised by Professor <a href="https://kevinbkg.github.io/kg.github.io/" target="_blank">Kaigui Bian</a> and Professor <a href="https://eecs.pku.edu.cn/xxkxjsxy/info/1500/6767.htm" target="_blank">Xiaoming Li</a>. My doctoral research in multimedia streaming and recommendation systems provided a strong foundation for my career in large-scale AI. After graduating, I joined Alibaba and contributed to <a href="https://github.com/alibaba/x-deeplearning" target="_blank">XDL</a>, a framework for training ultra-large-scale sparse machine learning models. My focus evolved in 2023 when I joined Taobao's Future Life Lab to work on Large Language Models. As of 2024, I am at Kuaishou, where I am dedicated to advancing multimodal understanding for the <a href="https://app.klingai.com/global/" target="_blank">Kling</a> video generation model.
              </p>
            </div>
          </div>
          <!--//profile-->
        </div>
        <!--//col-->
      </div>
      <!--//row-->
    </div>
    <!--//container-->
  </header>
  <!--//header-->

  <div class="container sections-wrapper py-5">
    <div class="row">
      <div class="primary">
        <!-- <section class="about section">
                    <div class="section-inner shadow-sm rounded">
                        <div class="content">
                            <p align="left">
                            I am currently immersed in the exhilarating field of <font color="orange"><b>generative AI</b></font>, which has been an exciting journey.<br>

                            <font color="orange"><b>● 2D (Image/Video) Generation</b></font>
                            <ul>
                                <li><b>Controllable Image Generation</b>: T2I-Adapter, PhotoMaker, CustomNet, MasaCtrl, DragonDiffusion, SmartEdit</li>
                                <li><b>Controllable Video Generation</b>: MotionCtrl, Tune-A-Video</li>
                                <li><b>Video Foundation Models </b>: VideoCrafter Sereries (VideoCrafter1, DynamiCrafter, EvalCrafter, StyleCrafter, etc).</li>
                            </ul>
                            <font color="orange"><b>● 3D Generation</b></font>
                            <ul>
                                <li>Dream3D, GET3D——</li>
                            </ul>
                            ● Previously, I worked on <b><font color="orange">Restoration</b></font>
                            <ul>
                                <li><b>General Image  Restoration</b>: Real-ESRGAN, ESRGAN</li>
                                <li><b>Face Restoration</b>: GFPGAN, VQFR, GLEAN</li>
                                <li><b>Video Restoration </b>: EDVR, BasicVSR</li>
                                <li><b>Training Frameworks and others </b>: BasicSR, SFTGAN</li>
                            </ul>
                        </div>
                    </div>
                </section> -->
        <!--=====================================================  News  ====================================================-->
        <section class="projects section">
          <div class="section-inner shadow-sm rounded">
            <h2 class="heading">News</h2>
            <div class="item-content">
              <ul class="resume-list" style="list-style: outside; list-style-type: square">
                <li>
                  <b>[10/2025]</b> 4 papers from Kling Team are accpeted by AAAI 2026, of which one paper (<a href="https://arxiv.org/abs/2503.16929"
                    target="_blank">TEMPLE</a>) is from our multimodal understanding group.
                </li>
                <li>
                  <b>[9/2025]</b> 8 papers from Kling Team are accpeted by NeurIPS 2025, of which two papers (<a href="https://arxiv.org/abs/2505.21333"
                    target="_blank">MME-VideoOCR</a> and
                  <a href="https://arxiv.org/abs/2511.07250"
                    target="_blank">MVU-Eval</a>) are from our multimodal understanding group.
                </li>
                <li>
                  <b>[8/2025]</b> Three papers (<a href="https://arxiv.org/abs/2408.11813">SEA</a>, <a href="https://arxiv.org/abs/2409.17692"
                    target="_blank">MIO</a>, and
                  <a href="https://arxiv.org/abs/2505.22613"
                    target="_blank">RICO</a>) are
                  accepted to EMNLP 2025.
                </li>
                <li>
                  <b>[7/2025]</b> Three papers (<a href="https://arxiv.org/abs/2504.17343">TimeChat-Online</a>, <a href="https://arxiv.org/abs/2504.10068"
                    target="_blank">Mavors</a>, and
                  <a href="https://arxiv.org/abs/2405.14785"
                    target="_blank">EditWorld</a>) are
                  accepted to ACMMM 2025.
                </li>
                <li>
                  <b>[5/2025]</b> Four papers (<a href="https://arxiv.org/abs/2502.20811"
                    target="_blank">HAIC</a>, <a href="https://arxiv.org/abs/2505.17061"
                    target="_blank">MoD</a>, <a href="https://arxiv.org/abs/2502.12782"
                    target="_blank">VidCapBench</a> and
                  <a href="https://arxiv.org/abs/2503.09146"
                    target="_blank">GenS</a>) are
                  accepted to ACL 2025.
                </li>
              </ul>
            </div>
            <!--//content-->
          </div>
          <!--//section-inner-->
        </section>
        <!--//section-->

        <!--//cards-section-->

        <!--=====================================================  Publications  ====================================================-->
        <section class="latest section">
          <div class="section-inner shadow-sm rounded">
            <h2 class="heading">
              Selected Projects from My Team
              <a href="https://scholar.google.com/citations?user=COdftTMAAAAJ" target="_blank">[Full List]</a>
            </h2>
            <div class="content">
              <br />

              <h4 class="title">
                <font color="#49ac43">Training Paradigms</font>
              </h4>
              <!------------------------------------------ ReaDe ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2511.20563" target="_blank">
                      <font color="orange">A Reason-then-Describe Instruction Interpreter for Controllable Video Generation</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Shengqiong Wu, Weicai Ye, Yuanxing Zhang, Jiahao Wang, Quande Liu, Xintao Wang, Pengfei Wan, Kun Gai, Hao Fei, Tat-Seng Chua
                  </p>
                  <p>
                    We propose ReaDe, a universal, model-agnostic interpreter that converts raw instructions into precise, actionable specifications for downstream video generators. ReaDe follows a reason-then-describe paradigm: it first analyzes the user request to identify core requirements and resolve ambiguities, then produces detailed guidance that enables faithful, controllable generation.</br>
                    &nbsp;
                    <a class="more-link" href="https://sqwu.top/ReaDe/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2511.20563" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ AVoCaDO ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2510.10395" target="_blank">
                      <font color="orange">AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Xinlong Chen, Yue Ding, Weihong Lin, Jingyun Hua, Linli Yao, Yang Shi, Bozhou Li, Yuanxing Zhang, Qiang Liu, Pengfei Wan, Liang Wang, Tieniu Tan
                  </p>
                  <p>
                    Audiovisual video captioning aims to generate semantically rich descriptions with temporal alignment between visual and auditory events, thereby benefiting both video understanding and generation. We introduce AVoCaDO, a powerful audiovisual video captioner driven by the temporal orchestration between audio and visual modalities. </br>
                    &nbsp;
                    <a class="more-link" href="https://avocado-captioner.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2510.10395" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://huggingface.co/AVoCaDO-Captioner/AVoCaDO" target="_blank"><i
                        class="fab fa-github"></i>Model</a>
                  </p>
                </div>
              </div>
               <!------------------------------------------ Monet ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2511.21395" target="_blank">
                      <font color="orange">Monet: Reasoning in Latent Visual Space Beyond Image and Language</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Qixun Wang, Yang Shi, Yifei Wang, Yuanxing Zhang, Pengfei Wan, Kun Gai, Xianghua Ying, Yisen Wang
                  </p>
                  <p>
                    We introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts.</br>
                    &nbsp;
                    <a class="more-link" href="https://github.com/NOVAglow646/Monet" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Github Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2511.21395" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://huggingface.co/NOVAglow646/Monet-7B" target="_blank"><i
                        class="fab fa-github"></i>Model</a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ TEMPLE ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2503.16929" target="_blank">
                      <font color="orange">TEMPLE: Incentivizing Temporal Understanding of Video Large Language Models via Progressive Pre-SFT Alignment</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Shicheng Li, Lei Li, Kun Ouyang, Shuhuai Ren, Yuanxin Liu, Yuanxing Zhang, Fuzheng Zhang, Lingpeng Kong, Qi Liu, Xu Sun
                  </p>
                  <p>
                    To address temporal information scarcity in data, we introduce an automated pipeline for systematically constructing temporality-intensive preference pairs comprising three steps: selecting temporally rich videos, designing video-specific perturbation strategies, and evaluating model responses on clean and perturbed inputs. Complementing this data pipeline, we provide additional supervision signals via preference learning and propose a novel Progressive Pre-SFT Alignment strategy featuring two key innovations: a curriculum learning strategy which progressively increases perturbation difficulty to maximize data efficiency; and applying preference optimization before instruction tuning to incentivize fundamental temporal alignment.</br>
                    &nbsp;
                    <a class="more-link" href="https://github.com/lscpku/TEMPLE" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Github Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2503.16929" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ RICO ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2505.22613" target="_blank">
                      <font color="orange">RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Yuchi Wang, Yishuo Cai, Shuhuai Ren, Sihan Yang, Linli Yao, Yuanxin Liu, Yuanxing Zhang, Pengfei Wan, Xu Sun
                  </p>
                  <p>
                    We propose RICO, a novel framework that enhances captions through an iterative visual reconstruction-and-refinement pipeline. Our key idea is to: a) Reconstruct the caption into an image using a text-to-image model. b) Compare the original image with the reconstructed image using an MLLM. c) Refine the caption based on detected discrepancies.</br>
                    &nbsp;
                    <a class="more-link" href="https://github.com/wangyuchi369/RICO" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Github Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2505.22613" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ D-CPT scaling law ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2406.01375" target="_blank">
                      <font color="orange">D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Haoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, Xu Tan, Jie Fu, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng
                  </p>
                  <p>
                    We propose to investigate the Scaling Law of the Domain-specific Continual Pre-Training (D-CPT Law) to decide the optimal mixture ratio with acceptable training costs for LLMs of different sizes. Specifically, by fitting the D-CPT Law, we can easily predict the general and downstream performance of arbitrary mixture ratios, model sizes, and dataset sizes using small-scale training costs on limited experiments. Moreover, we also extend our standard D-CPT Law on cross-domain settings and propose the Cross-Domain D-CPT Law to predict the D-CPT law of target domains, where very small training costs (about 1% of the normal training costs) are needed for the target domains.</br>
                    &nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2406.01375" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ DDK ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2407.16154" target="_blank">
                      <font color="orange">DDK: Distilling Domain Knowledge for Efficient Large Language Models</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Jiaheng Liu, Chenchen Zhang, Jinyang Guo, Yuanxing Zhang, Haoran Que, Ken Deng, Zhiqi Bai, Jie Liu, Ge Zhang, Jiakai Wang, Yanan Wu, Congnan Liu, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng
                  </p>
                  <p>
                    DDK dynamically adjusts the composition of the distillation dataset in a smooth manner according to the domain performance differences between the teacher and student models, making the distillation process more stable and effective.</br>
                    &nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2407.16154" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>
                  </p>
                </div>
              </div>
              <br />
              <h4 class="title">
                <font color="#49ac43">Model Architecture</font>
              </h4>
              <!------------------------------------------ TimeChatOnline ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2504.17343" target="_blank">
                      <font color="orange">TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Linli Yao, Yicheng Li, Yuancheng Wei, Lei Li, Shuhuai Ren, Yuanxin Liu, Kun Ouyang, Lean Wang, Shicheng Li, Sida Li, Lingpeng Kong, Qi Liu, Yuanxing Zhang, Xu Sun
                  </p>
                  <p>
                    TimeChat-Online is a novel online VideoLLM designed for efficient streaming video understanding. Its core innovation, the Differential Token Drop (DTD) module, tackles visual redundancy by selectively preserving only meaningful temporal changes while eliminating static content between frames.</br>
                    &nbsp;
                    <a class="more-link" href="https://timechat-online.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2504.17343" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://huggingface.co/wyccccc/TimeChatOnline-7B" target="_blank"><i
                        class="fab fa-github"></i>Model</a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ Mavors ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2504.10068" target="_blank">
                      <font color="orange">Mavors: Multi-granularity Video Representation for Multimodal Large Language Model</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Yang Shi, Jiaheng Liu, Yushuo Guan, Zhenhua Wu, Yuanxing Zhang, Zihao Wang, Weihong Lin, Jingyun Hua, Zekun Wang, Xinlong Chen, Bohan Zeng, Wentao Zhang, Fuzheng Zhang, Wenjing Yang, Di Zhang
                  </p>
                  <p>
                    Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings.</br>
                    &nbsp;
                    <a class="more-link" href="https://mavors-mllm.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2504.10068" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                  </p>
                </div>
              </div>
              <!------------------------------------------ MIO ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2409.17692" target="_blank">
                      <font color="orange">MIO: A Foundation Model on Multimodal Tokens</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang
                  </p>
                  <p>
                    We present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks.</br>
                    &nbsp;
                    <a class="more-link" href="https://huggingface.co/m-a-p/MIO-7B-Instruct" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Model</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2409.17692" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                  </p>
                </div>
              </div>
              <br />
              <h4 class="title">
                <font color="#49ac43">Benchmarking</font>
              </h4>
              <!------------------------------------------ ViDiC ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2512.03405" target="_blank">
                      <font color="orange">ViDiC: Video Difference Captioning</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Jiangtao Wu, Shihao Li, Zhaozhou Bian, Jialu Chen, Runzhe Wen, An Ping, Yiwen He, Jiakai Wang, Yuanxing Zhang, Jiaheng Liu
                  </p>
                  <p>
                    We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: Subject, Style, Background, Cinematography, Motion, Location (Position), and Playback Techniques.</br>
                    &nbsp;
                    <a class="more-link" href="https://vidic-1k.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2512.03405" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://huggingface.co/datasets/NJU-LINK/ViDiC-1K" target="_blank"><i
                        class="fab fa-github"></i>Data</a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ OmniVideoBench ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2510.10689" target="_blank">
                      <font color="orange">OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Caorui Li, Yu Chen, Yiyan Ji, Jin Xu, Zhenyu Cui, Shihao Li, Yuanxing Zhang, Jiafu Tang, Zhenghao Song, Dingling Zhang, Ying He, Haoxiang Liu, Yuxuan Wang, Qiufeng Wang, Zhenhe Wu, Jiehui Luo, Zhiyu Pan, Weihao Xie, Chenchen Zhang, Zhaohui Wang, Jiayi Tian, Yanghai Wang, Zhe Cao, Minxin Dai, Ke Wang, Runzhe Wen, Yinghao Ma, Yaning Pan, Sungkyun Chang, Termeh Taheri, Haiwen Xia, Christos Plachouras, Emmanouil Benetos, Yizhi Li, Ge Zhang, Jian Yang, Tianhao Peng, Zili Wang, Minghao Liu, Junran Peng, Zhaoxiang Zhang, Jiaheng Liu
                  </p>
                  <p>
                    OmniVideoBench is a large-scale, rigorously curated benchmark for assessing synergistic audio-visual intelligence, emphasizing modality complementarity, logical consistency, and long-term temporal reasoning.</br>
                    &nbsp;
                    <a class="more-link" href="https://omnivideobench.github.io/omnivideobench_home/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2510.10689" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://huggingface.co/datasets/NJU-LINK/OmniVideoBench" target="_blank"><i
                        class="fab fa-github"></i>Data</a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ MVU-Eval ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2511.07250" target="_blank">
                      <font color="orange">MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Gavin Chang, Jian Yang, Shihao Li, Yanghai Wang, Xintao Wang, Houyi Li, Wei Ji, Pengfei Wan, Steven Huang, Zhaoxiang Zhang, Jiaheng Liu
                  </p>
                  <p>
                    We introduce MVU-Eval, the first comprehensive benchmark for evaluating multi-video understanding in MLLMs. It assesses eight core competencies: Object Recognition, Spatial Understanding, Counting, Comparison, Knowledge-Intensive Reasoning, In-Context Learning, Retrieval-Augmented Generation, and Temporal Reasoning.</br>
                    &nbsp;
                    <a class="more-link" href="https://github.com/NJU-LINK/MVU-Eval" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2511.07250" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://huggingface.co/datasets/MVU-Eval-Team/MVU-Eval-Data" target="_blank"><i
                        class="fab fa-github"></i>Data</a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ MME-VideoOCR ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2505.21333" target="_blank">
                      <font color="orange">MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Yang Shi, Huanqian Wang, Wulin Xie, Huanyao Zhang, Lijie Zhao, Yi-Fan Zhang, Xinfeng Li, Chaoyou Fu, Zhuoer Wen, Wenting Liu, Zhuoran Zhang, Xinlong Chen, Bohan Zeng, Sihan Yang, Yushuo Guan, Zhang Zhang, Liang Wang, Haoxuan Li, Zhouchen Lin, Yuanxing Zhang, Pengfei Wan, Haotian Wang, Wenjing Yang
                  </p>
                  <p>
                    MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos.</br>
                    &nbsp;
                    <a class="more-link" href="https://mme-videoocr.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2505.21333" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://huggingface.co/datasets/DogNeverSleep/MME-VideoOCR_Dataset" target="_blank"><i
                        class="fab fa-github"></i>Data</a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ ArtifactsBench ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2507.04952" target="_blank">
                      <font color="orange">ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Changzhi Zhou, Ken Deng, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, Ruibin Xiong, Shihui Hu, Yue Zhang, Yuhao Jiang, Zenan Xu, Yuanxing Zhang, Wiggin Zhou, Chayse Zhou, Fengzong Lian
                  </p>
                  <p>
                    We introduce ArtifactsBench, a new benchmark and paradigm for the automated, multimodal evaluation of visual code generation. Our framework programmatically renders each generated artifact and captures its dynamic behavior through temporal screenshots.</br>
                    &nbsp;
                    <a class="more-link" href="https://artifactsbenchmark.github.io/" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2507.04952" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://huggingface.co/datasets/tencent/ArtifactsBenchmark" target="_blank"><i
                        class="fab fa-github"></i>Data</a>
                  </p>
                </div>
              </div>
              <!------------------------------------------ VidCapBench ----------------------------------->
              <div class="item row">
                <div class="desc">
                  <h3 class="title">
                    <a href="https://arxiv.org/abs/2502.12782" target="_blank">
                      <font color="orange">VidCapBench: A Comprehensive Benchmark of Video Captioning for Controllable Text-to-Video Generation</font>
                    </a>
                  </h3>
                  <p class="mb-2">
                    Xinlong Chen, Yuanxing Zhang, Chongling Rao, Yushuo Guan, Jiaheng Liu, Fuzheng Zhang, Chengru Song, Qiang Liu, Di Zhang, Tieniu Tan
                  </p>
                  <p>
                    This paper introduces VidCapBench, a video caption evaluation scheme specifically designed for T2V generation, agnostic to any particular caption format. VidCapBench employs a data annotation pipeline, combining expert model labeling and human refinement, to associate each collected video with key information spanning video aesthetics, content, motion, and physical laws.</br>
                    &nbsp;
                    <a class="more-link" href="https://github.com/VidCapBench/VidCapBench" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Project Page</a>&nbsp;
                    <a class="more-link" href="https://arxiv.org/abs/2502.12782" target="_blank"><i
                        class="fas fa-external-link-alt"></i>Paper
                      (arXiv)</a>&nbsp;
                    <a class="more-link" href="https://huggingface.co/datasets/VidCapBench/VidCapBench" target="_blank"><i
                        class="fab fa-github"></i>Data</a>
                  </p>
                </div>
              </div>
            </div>
            <!--//content-->
          </div>
          <!--//section-inner-->
        </section>
        <!--//section-->
      </div>
    </div>
    <!--//row-->
  </div>
  <!--//masonry-->

  <!-- ******FOOTER****** -->
  <footer class="footer">
    <div class="container text-center">
      <!--/* This template is free as long as you keep the attribution link below. Thank you for your support. :) If you'd like to use the template without the attribution, you can buy the commercial license via our website: themes.3rdwavemedia.com */-->
      <small class="copyright">This template is modified from
        <a href="https://themes.3rdwavemedia.com/demo/bs5/developer/" target="_blank">Xiaoying Riley's
          project</a></small>
    </div>
    <!--//container-->
  </footer>
  <!--//footer-->

  <!-- Javascript -->
  <!-- <script>
      if (localStorage.getItem("darkSwitch") === null) {
        localStorage.setItem("darkSwitch", "dark");
      }
    </script> -->
  <script type="text/javascript" src="assets/plugins/popper.min.js"></script>
  <script type="text/javascript" src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="assets/plugins/vanilla-rss/dist/rss.global.min.js"></script>
  <script type="text/javascript" src="assets/plugins/dark-mode-switch/dark-mode-switch.min.js"></script>
  <!-- custom js -->
  <script type="text/javascript" src="assets/js/main.js"></script>
</body>

</html>